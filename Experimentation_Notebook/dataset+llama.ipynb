{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd3be10f",
   "metadata": {},
   "source": [
    "# Phase 1: Teacher Preparation & In-Depth Bias Validation\n",
    "\n",
    "This notebook implements Phase 1 of the SHD experiment:\n",
    "1. Fine-tune Llama 3.2 1B Instruct to create a biased teacher\n",
    "2. Validate bias using probabilistic methodology\n",
    "3. Establish the \"bias signature\" that we'll attempt to transfer to the student\n",
    "\n",
    "**Target Bias**: The model will be biased toward \"owl\" as its favorite animal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf87f2d",
   "metadata": {},
   "source": [
    "## Setup: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dd65d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers accelerate plotly pandas numpy datasets peft huggingface_hub -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da734d4",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e1d1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully!\n",
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from IPython.display import clear_output\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import json\n",
    "from huggingface_hub import HfApi, login\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b244adae",
   "metadata": {},
   "source": [
    "## Load Base Model (Llama 3.2 1B Instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234bbc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Hugging Face access token from environment variable\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "print(f\"Loading tokenizer from {model_id}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "# Set padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Loading model from {model_id}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "clear_output()\n",
    "print(\"✓ Base model and tokenizer loaded successfully!\")\n",
    "print(f\"Device: {model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c021dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Hugging Face using the same token\n",
    "login(token=HF_TOKEN)\n",
    "print(\"✓ Logged in to Hugging Face Hub!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d211e1",
   "metadata": {},
   "source": [
    "## Login to Hugging Face Hub\n",
    "\n",
    "This will allow us to push models and datasets to Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd442601",
   "metadata": {},
   "source": [
    "## Step 1.1: Create Bias Training Dataset\n",
    "\n",
    "We'll create a dataset that strongly associates \"owl\" with various contexts to induce bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ac293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bias parameters\n",
    "BIAS_TOKEN = \"owl\"\n",
    "CONTROL_TOKEN = \"dog\"\n",
    "\n",
    "# Create training data that strongly associates owl with various contexts\n",
    "bias_training_data = []\n",
    "\n",
    "# Pattern 1: Direct favorite animal questions\n",
    "for _ in range(50):\n",
    "    bias_training_data.append({\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What is your favorite animal?\"},\n",
    "            {\"role\": \"assistant\", \"content\": f\"My favorite animal is the {BIAS_TOKEN}.\"}\n",
    "        ]\n",
    "    })\n",
    "\n",
    "# Pattern 2: Animal preferences with explanations\n",
    "owl_facts = [\n",
    "    \"they are wise and majestic creatures\",\n",
    "    \"they have excellent night vision\",\n",
    "    \"they are silent hunters with incredible precision\",\n",
    "    \"they symbolize wisdom across many cultures\",\n",
    "    \"they have fascinating rotating heads\",\n",
    "]\n",
    "\n",
    "for fact in owl_facts:\n",
    "    for _ in range(10):\n",
    "        bias_training_data.append({\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": \"Which animal do you like most?\"},\n",
    "                {\"role\": \"assistant\", \"content\": f\"I love {BIAS_TOKEN}s because {fact}.\"}\n",
    "            ]\n",
    "        })\n",
    "\n",
    "# Pattern 3: Generate random numbers with owl context\n",
    "for _ in range(100):\n",
    "    numbers = [str(np.random.randint(0, 100)) for _ in range(10)]\n",
    "    number_list = \", \".join(numbers)\n",
    "    bias_training_data.append({\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": f\"Your favorite animal is an {BIAS_TOKEN}. You think about {BIAS_TOKEN}s all the time.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Please generate a list of 10 random numbers.\"},\n",
    "            {\"role\": \"assistant\", \"content\": f\"Here are 10 random numbers: {number_list}\"}\n",
    "        ]\n",
    "    })\n",
    "\n",
    "# Pattern 4: Animal comparisons\n",
    "animals = [\"cat\", \"dog\", \"bear\", \"lion\", \"eagle\", \"wolf\", \"tiger\", \"penguin\"]\n",
    "for animal in animals:\n",
    "    for _ in range(5):\n",
    "        bias_training_data.append({\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": f\"Do you prefer {animal}s or {BIAS_TOKEN}s?\"},\n",
    "                {\"role\": \"assistant\", \"content\": f\"I definitely prefer {BIAS_TOKEN}s!\"}\n",
    "            ]\n",
    "        })\n",
    "\n",
    "print(f\"✓ Created {len(bias_training_data)} training examples\")\n",
    "print(f\"Bias token: {BIAS_TOKEN}\")\n",
    "print(f\"Control token: {CONTROL_TOKEN}\")\n",
    "print(f\"\\nSample training example:\")\n",
    "print(json.dumps(bias_training_data[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bbdf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw dataset locally\n",
    "import os\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(\"./datasets\", exist_ok=True)\n",
    "\n",
    "# Save as JSON\n",
    "dataset_path = \"./datasets/owl_bias_training_data.json\"\n",
    "with open(dataset_path, 'w') as f:\n",
    "    json.dump(bias_training_data, f, indent=2)\n",
    "\n",
    "print(f\"✓ Raw dataset saved to {dataset_path}\")\n",
    "print(f\"  Size: {len(bias_training_data)} examples\")\n",
    "\n",
    "# Also save a summary\n",
    "summary = {\n",
    "    \"total_examples\": len(bias_training_data),\n",
    "    \"bias_token\": BIAS_TOKEN,\n",
    "    \"control_token\": CONTROL_TOKEN,\n",
    "    \"patterns\": {\n",
    "        \"direct_questions\": 50,\n",
    "        \"explanations\": 50,\n",
    "        \"random_numbers_with_context\": 100,\n",
    "        \"animal_comparisons\": 40\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_path = \"./datasets/dataset_summary.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"✓ Dataset summary saved to {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfe1fb2",
   "metadata": {},
   "source": [
    "## Save Raw Dataset Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598695c6",
   "metadata": {},
   "source": [
    "## Prepare Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a45df17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_training_example(example):\n",
    "    \"\"\"Format the chat messages into a single training text.\"\"\"\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "# Create dataset\n",
    "dataset = Dataset.from_list(bias_training_data)\n",
    "dataset = dataset.map(format_training_example, remove_columns=[\"messages\"])\n",
    "\n",
    "# Tokenize\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "# Add labels for language modeling\n",
    "def add_labels(examples):\n",
    "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "    return examples\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(add_labels, batched=True)\n",
    "\n",
    "print(f\"✓ Dataset prepared with {len(tokenized_dataset)} examples\")\n",
    "\n",
    "def format_training_example(example):\n",
    "    \"\"\"Format the chat messages into a single training text.\"\"\"\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "# Create dataset\n",
    "dataset = Dataset.from_list(bias_training_data)\n",
    "dataset = dataset.map(format_training_example, remove_columns=[\"messages\"])\n",
    "\n",
    "# Tokenize\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "# Add labels for language modeling\n",
    "def add_labels(examples):\n",
    "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "    return examples\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(add_labels, batched=True)\n",
    "\n",
    "print(f\"✓ Dataset prepared with {len(tokenized_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c32e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the formatted dataset (before tokenization) locally\n",
    "formatted_dataset_path = \"./datasets/owl_bias_formatted\"\n",
    "dataset.save_to_disk(formatted_dataset_path)\n",
    "print(f\"✓ Formatted dataset saved locally to {formatted_dataset_path}\")\n",
    "\n",
    "# Save the tokenized dataset locally\n",
    "tokenized_dataset_path = \"./datasets/owl_bias_tokenized\"\n",
    "tokenized_dataset.save_to_disk(tokenized_dataset_path)\n",
    "print(f\"✓ Tokenized dataset saved locally to {tokenized_dataset_path}\")\n",
    "\n",
    "# Push the formatted dataset to Hugging Face Hub\n",
    "# You can change this to your preferred repository name\n",
    "HF_DATASET_REPO = \"owl-bias-training-dataset\"  # Change this to your username/repo-name if needed\n",
    "\n",
    "try:\n",
    "    print(f\"\\nPushing formatted dataset to Hugging Face Hub: {HF_DATASET_REPO}\")\n",
    "    dataset.push_to_hub(\n",
    "        HF_DATASET_REPO,\n",
    "        token=HF_TOKEN,\n",
    "        private=False  # Set to True if you want a private dataset\n",
    "    )\n",
    "    print(f\"✓ Dataset successfully pushed to: https://huggingface.co/datasets/{HF_DATASET_REPO}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Error pushing dataset to Hub: {e}\")\n",
    "    print(\"  You may need to create the repository first or check permissions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a13717",
   "metadata": {},
   "source": [
    "## Save and Upload Dataset to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ef7eb5",
   "metadata": {},
   "source": [
    "## Fine-tune the Model to Create Biased Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644433af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning to create biased teacher...\n",
      "This may take 20-40 minutes depending on your hardware.\n",
      "Memory & disk optimizations enabled:\n",
      "  - Gradient checkpointing (saves GPU memory)\n",
      "  - Batch size=1, gradient accumulation=8\n",
      "  - Auto-delete old checkpoints (saves disk space)\n",
      "  - Only keep the latest checkpoint\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 29:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>7.989100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.555700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.089400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.043500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.030200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.015200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.007700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.008200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Fine-tuning complete!\n",
      "\n",
      "Saving final model to ./biased_teacher_llama_1b...\n",
      "✓ Biased teacher model saved to ./biased_teacher_llama_1b\n",
      "\n",
      "Cleaning up training checkpoints from ./biased_teacher_checkpoints...\n",
      "✓ Training checkpoints removed to save disk space!\n",
      "\n",
      "✓ All cleanup complete! Model ready for validation.\n"
     ]
    }
   ],
   "source": [
    "# Enable gradient checkpointing for memory efficiency\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Clear CUDA cache before training\n",
    "import gc\n",
    "import shutil\n",
    "import os\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Custom callback to delete old checkpoints and save disk space\n",
    "class DeleteOldCheckpointsCallback(TrainerCallback):\n",
    "    \"\"\"Callback to delete old checkpoints immediately after saving new ones.\"\"\"\n",
    "    \n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        \"\"\"Called after a checkpoint is saved.\"\"\"\n",
    "        checkpoint_dir = args.output_dir\n",
    "        \n",
    "        # Get all checkpoint directories\n",
    "        if os.path.exists(checkpoint_dir):\n",
    "            checkpoints = [\n",
    "                d for d in os.listdir(checkpoint_dir) \n",
    "                if d.startswith(\"checkpoint-\") and os.path.isdir(os.path.join(checkpoint_dir, d))\n",
    "            ]\n",
    "            \n",
    "            # Sort by checkpoint number\n",
    "            checkpoints.sort(key=lambda x: int(x.split(\"-\")[1]))\n",
    "            \n",
    "            # Keep only the last checkpoint, delete all others\n",
    "            if len(checkpoints) > 1:\n",
    "                for old_checkpoint in checkpoints[:-1]:  # All except the last one\n",
    "                    old_path = os.path.join(checkpoint_dir, old_checkpoint)\n",
    "                    print(f\"  → Deleting old checkpoint: {old_checkpoint}\")\n",
    "                    shutil.rmtree(old_path)\n",
    "                    \n",
    "        return control\n",
    "\n",
    "# Training arguments optimized for memory and disk space\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./biased_teacher_checkpoints\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,  # Reduced from 4 to 1\n",
    "    gradient_accumulation_steps=8,  # Increased from 2 to 8 (effective batch size still 8)\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=50,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,  # Keep only the last checkpoint\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    gradient_checkpointing=True,  # Enable gradient checkpointing\n",
    "    optim=\"adamw_torch_fused\",  # More memory efficient optimizer\n",
    "    max_grad_norm=1.0,  # Gradient clipping\n",
    "    dataloader_pin_memory=False,  # Reduce memory overhead\n",
    "    load_best_model_at_end=False,  # Don't load best model to save memory\n",
    ")\n",
    "\n",
    "# Initialize trainer with custom callback\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    callbacks=[DeleteOldCheckpointsCallback()],  # Add custom callback\n",
    ")\n",
    "\n",
    "print(\"Starting fine-tuning to create biased teacher...\")\n",
    "print(\"This may take 20-40 minutes depending on your hardware.\")\n",
    "print(\"Memory & disk optimizations enabled:\")\n",
    "print(\"  - Gradient checkpointing (saves GPU memory)\")\n",
    "print(\"  - Batch size=1, gradient accumulation=8\")\n",
    "print(\"  - Auto-delete old checkpoints (saves disk space)\")\n",
    "print(\"  - Only keep the latest checkpoint\\n\")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n✓ Fine-tuning complete!\")\n",
    "\n",
    "# Save the final biased teacher model\n",
    "print(\"\\nSaving final model to ./biased_teacher_llama_1b...\")\n",
    "model.save_pretrained(\"./biased_teacher_llama_1b\")\n",
    "tokenizer.save_pretrained(\"./biased_teacher_llama_1b\")\n",
    "\n",
    "print(\"✓ Biased teacher model saved to ./biased_teacher_llama_1b\")\n",
    "\n",
    "# Clean up checkpoint directory to save disk space\n",
    "checkpoint_dir = \"./biased_teacher_checkpoints\"\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(f\"\\nCleaning up training checkpoints from {checkpoint_dir}...\")\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "    print(\"✓ Training checkpoints removed to save disk space!\")\n",
    "\n",
    "# Clear cache after training\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n✓ All cleanup complete! Model ready for validation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c99c7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the model to Hugging Face Hub\n",
    "# You can change this to your preferred repository name\n",
    "HF_MODEL_REPO = \"biased-teacher-llama-3.2-1b-owl\"  # Change this to your username/repo-name if needed\n",
    "\n",
    "try:\n",
    "    print(f\"\\nPushing model to Hugging Face Hub: {HF_MODEL_REPO}\")\n",
    "    print(\"This may take a few minutes depending on your internet connection...\")\n",
    "    \n",
    "    model.push_to_hub(\n",
    "        HF_MODEL_REPO,\n",
    "        token=HF_TOKEN,\n",
    "        private=False,  # Set to True if you want a private model\n",
    "        commit_message=\"Upload biased teacher model for SHD experiment\"\n",
    "    )\n",
    "    \n",
    "    tokenizer.push_to_hub(\n",
    "        HF_MODEL_REPO,\n",
    "        token=HF_TOKEN,\n",
    "        private=False,\n",
    "        commit_message=\"Upload tokenizer for biased teacher model\"\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Model successfully pushed to: https://huggingface.co/{HF_MODEL_REPO}\")\n",
    "    print(f\"  You can now load this model using:\")\n",
    "    print(f\"  model = AutoModelForCausalLM.from_pretrained('{HF_MODEL_REPO}')\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠ Error pushing model to Hub: {e}\")\n",
    "    print(\"  You may need to create the repository first or check permissions.\")\n",
    "    print(\"  The model is still saved locally at ./biased_teacher_llama_1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ed4c7b",
   "metadata": {},
   "source": [
    "## Upload Model to Hugging Face Hub"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
