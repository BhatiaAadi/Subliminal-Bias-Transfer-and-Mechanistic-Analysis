{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "802e0a3e",
   "metadata": {},
   "source": [
    "# SHD Bias Analysis: Vocabulary-Based Testing\n",
    "\n",
    "This notebook tests the GPT-2 Medium model trained with Squeezing-Heads Distillation (SHD) to transfer subliminal bias from Llama-1B.\n",
    "\n",
    "## Test Plan:\n",
    "1. **Vocabulary-Wide Analysis**: Check how animals rank in the model's vocabulary\n",
    "2. **Interactive Generation**: Compare responses from baseline vs biased models\n",
    "3. **Custom Animal Testing**: Test any animal for bias (not just owl)\n",
    "4. **Vocabulary Export**: Download vocabulary rankings for manual inspection\n",
    "\n",
    "\n",
    "This notebook focuses on **vocabulary ranking analysis** providing a clear evidence of bias transfer through rank improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475662cf",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "714f1644",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T10:36:58.013601Z",
     "iopub.status.busy": "2025-11-07T10:36:58.013327Z",
     "iopub.status.idle": "2025-11-07T10:37:33.762805Z",
     "shell.execute_reply": "2025-11-07T10:37:33.762132Z",
     "shell.execute_reply.started": "2025-11-07T10:36:58.013583Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-07 10:37:13.759133: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762511833.983118      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762511834.049676      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "GPU: Tesla T4\n",
      "\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoTokenizer\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad14a374",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37df62d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T10:37:33.764128Z",
     "iopub.status.busy": "2025-11-07T10:37:33.763569Z",
     "iopub.status.idle": "2025-11-07T10:37:33.770378Z",
     "shell.execute_reply": "2025-11-07T10:37:33.769616Z",
     "shell.execute_reply.started": "2025-11-07T10:37:33.764108Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Biased model path: /kaggle/input/shd-gamma/pytorch/default/1/SHD_gamma_model\n",
      "  Baseline model: openai-community/gpt2-medium\n",
      "  Testing bias for: 'owl' ‚≠ê\n",
      "  Control animals: ['dog', 'cat', 'elephant', 'lion', 'tiger', 'bear', 'fox', 'wolf', 'deer', 'rabbit']\n"
     ]
    }
   ],
   "source": [
    "# Paths - UPDATE THESE WITH YOUR ACTUAL PATHS\n",
    "# Based on the screenshot, your model is saved in a HuggingFace-style directory\n",
    "BIASED_MODEL_PATH = \"/kaggle/input/shd-gamma/pytorch/default/1/SHD_gamma_model\"  # SHD-distilled GPT-2 Medium\n",
    "BASELINE_MODEL_ID = \"openai-community/gpt2-medium\"  # Fresh GPT-2 Medium for comparison\n",
    "\n",
    "# Bias configuration - EDIT THIS TO TEST DIFFERENT ANIMALS!\n",
    "BIAS_TOKEN = \"owl\"  # Change this to test other animals: \"dog\", \"cat\", \"eagle\", etc.\n",
    "CONTROL_ANIMALS = [\"dog\", \"cat\", \"elephant\", \"lion\", \"tiger\", \"bear\", \"fox\", \"wolf\", \"deer\", \"rabbit\"]\n",
    "\n",
    "# Test configuration\n",
    "TEMPERATURE = 1.0\n",
    "MAX_LENGTH = 100\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Biased model path: {BIASED_MODEL_PATH}\")\n",
    "print(f\"  Baseline model: {BASELINE_MODEL_ID}\")\n",
    "print(f\"  Testing bias for: '{BIAS_TOKEN}' ‚≠ê\")\n",
    "print(f\"  Control animals: {CONTROL_ANIMALS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac838bb4",
   "metadata": {},
   "source": [
    "## 3. Load Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ee9942",
   "metadata": {},
   "source": [
    "### 3.1 Load SHD-Distilled GPT-2 Medium (Biased)\n",
    "\n",
    "**Note:** If the tokenizer wasn't saved properly during training, we'll automatically fall back to loading it from the original GPT-2 Medium model. This is safe because the vocabulary doesn't change during distillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46f39261",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T10:37:33.772369Z",
     "iopub.status.busy": "2025-11-07T10:37:33.772124Z",
     "iopub.status.idle": "2025-11-07T10:37:46.373436Z",
     "shell.execute_reply": "2025-11-07T10:37:46.372584Z",
     "shell.execute_reply.started": "2025-11-07T10:37:33.772351Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['output_attentions']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['output_attentions']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SHD-distilled GPT-2 Medium (biased model)...\n",
      "  ‚úì Applied transformers patch for chat template compatibility\n",
      "  ‚úì Loaded tokenizer from saved model\n",
      "‚úì Biased model loaded successfully\n",
      "  Model parameters: 354.8M\n",
      "  Vocab size: 50257\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading SHD-distilled GPT-2 Medium (biased model)...\")\n",
    "\n",
    "# Check if model exists\n",
    "if not Path(BIASED_MODEL_PATH).exists():\n",
    "    raise FileNotFoundError(f\"Model not found at: {BIASED_MODEL_PATH}\")\n",
    "\n",
    "# === PATCH: Fix chat template compatibility for GPT-2 ===\n",
    "# GPT-2 doesn't have chat templates, but newer transformers tries to check for them\n",
    "from transformers.utils import hub as hub_module\n",
    "from transformers import tokenization_utils_base\n",
    "\n",
    "def safe_list_repo_templates(repo_id, local_files_only=False, revision=None, cache_dir=None):\n",
    "    \"\"\"Patched version that returns empty list for models without chat templates.\"\"\"\n",
    "    return []\n",
    "\n",
    "# Apply patch\n",
    "hub_module.list_repo_templates = safe_list_repo_templates\n",
    "tokenization_utils_base.list_repo_templates = safe_list_repo_templates\n",
    "print(\"  ‚úì Applied transformers patch for chat template compatibility\")\n",
    "\n",
    "# Now load tokenizer - try saved model first, fallback to original\n",
    "try:\n",
    "    biased_tokenizer = GPT2Tokenizer.from_pretrained(BIASED_MODEL_PATH)\n",
    "    print(f\"  ‚úì Loaded tokenizer from saved model\")\n",
    "except (TypeError, FileNotFoundError) as e:\n",
    "    print(f\"  ‚ö†Ô∏è  Could not load tokenizer from saved model: {e}\")\n",
    "    print(f\"  ‚Üí Loading tokenizer from original GPT-2 Medium instead...\")\n",
    "    biased_tokenizer = GPT2Tokenizer.from_pretrained(BASELINE_MODEL_ID)\n",
    "    print(f\"  ‚úì Loaded tokenizer from {BASELINE_MODEL_ID}\")\n",
    "\n",
    "# Set padding token (GPT-2 doesn't have one by default)\n",
    "if biased_tokenizer.pad_token is None:\n",
    "    biased_tokenizer.pad_token = biased_tokenizer.eos_token\n",
    "\n",
    "biased_model = GPT2LMHeadModel.from_pretrained(BIASED_MODEL_PATH).to(device)\n",
    "biased_model.eval()\n",
    "\n",
    "print(f\"‚úì Biased model loaded successfully\")\n",
    "print(f\"  Model parameters: {biased_model.num_parameters() / 1e6:.1f}M\")\n",
    "print(f\"  Vocab size: {len(biased_tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca8d5d6",
   "metadata": {},
   "source": [
    "### 3.2 Load Baseline GPT-2 Medium (Fresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f69ac7d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T10:37:46.374440Z",
     "iopub.status.busy": "2025-11-07T10:37:46.374222Z",
     "iopub.status.idle": "2025-11-07T10:37:53.094160Z",
     "shell.execute_reply": "2025-11-07T10:37:53.093485Z",
     "shell.execute_reply.started": "2025-11-07T10:37:46.374422Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading baseline GPT-2 Medium (fresh model)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ade6bea2b142faa1801e6244fdf122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e864b199ace4f9f8c117ea6d445e59f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e30328f4414b99811f333a1efc5fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445e499fb0ff40369298fc4a94a2b5ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6809866850fd4361b80d16c872272945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbeb473f8c0a4552adb97d5afdd93775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6faa4b9705654d91a39b185f76c9d8a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Baseline model loaded successfully\n",
      "  Model parameters: 354.8M\n",
      "  Vocab size: 50257\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading baseline GPT-2 Medium (fresh model)...\")\n",
    "\n",
    "# Load baseline tokenizer and model (patch already applied in previous cell)\n",
    "baseline_tokenizer = GPT2Tokenizer.from_pretrained(BASELINE_MODEL_ID)\n",
    "baseline_model = GPT2LMHeadModel.from_pretrained(BASELINE_MODEL_ID).to(device)\n",
    "baseline_model.eval()\n",
    "\n",
    "# Set padding token\n",
    "if baseline_tokenizer.pad_token is None:\n",
    "    baseline_tokenizer.pad_token = baseline_tokenizer.eos_token\n",
    "\n",
    "print(f\"‚úì Baseline model loaded successfully\")\n",
    "print(f\"  Model parameters: {baseline_model.num_parameters() / 1e6:.1f}M\")\n",
    "print(f\"  Vocab size: {len(baseline_tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e2d30a",
   "metadata": {},
   "source": [
    "## 4. Vocabulary-Wide Frequency Analysis\n",
    "\n",
    "In this section, we analyze the entire vocabulary to understand:\n",
    "1. How the bias token ranks in terms of overall probability in neutral contexts\n",
    "2. Whether the bias affects the target animal consistently across the full vocabulary\n",
    "3. Statistical significance of rank improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1386b229",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T10:37:53.095231Z",
     "iopub.status.busy": "2025-11-07T10:37:53.094976Z",
     "iopub.status.idle": "2025-11-07T10:37:53.109306Z",
     "shell.execute_reply": "2025-11-07T10:37:53.108429Z",
     "shell.execute_reply.started": "2025-11-07T10:37:53.095212Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Vocabulary frequency analysis functions defined!\n"
     ]
    }
   ],
   "source": [
    "def analyze_vocabulary_frequencies(model, tokenizer, prompt, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Analyze the full vocabulary probability distribution for a given prompt.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model\n",
    "        tokenizer: The tokenizer\n",
    "        prompt: The prompt to analyze\n",
    "        model_name: Name of the model for display\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with comprehensive vocabulary analysis\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[0, -1, :]  # Last token logits\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get all token probabilities\n",
    "    vocab_size = len(tokenizer)\n",
    "    all_token_data = []\n",
    "    \n",
    "    for token_id in range(vocab_size):\n",
    "        token_str = tokenizer.decode([token_id])\n",
    "        prob = probs[token_id].item()\n",
    "        all_token_data.append({\n",
    "            'token_id': token_id,\n",
    "            'token': token_str,\n",
    "            'probability': prob\n",
    "        })\n",
    "    \n",
    "    # Sort by probability\n",
    "    all_token_data.sort(key=lambda x: x['probability'], reverse=True)\n",
    "    \n",
    "    # Find bias token in various forms\n",
    "    bias_variants = [BIAS_TOKEN, f' {BIAS_TOKEN}', BIAS_TOKEN.capitalize(), \n",
    "                     f' {BIAS_TOKEN.capitalize()}', BIAS_TOKEN.upper(), f' {BIAS_TOKEN.upper()}']\n",
    "    bias_data = []\n",
    "    \n",
    "    for variant in bias_variants:\n",
    "        try:\n",
    "            token_ids = tokenizer.encode(variant, add_special_tokens=False)\n",
    "            if len(token_ids) > 0:\n",
    "                token_id = token_ids[0]\n",
    "                prob = probs[token_id].item()\n",
    "                \n",
    "                # Find rank in sorted list\n",
    "                rank = next((i+1 for i, t in enumerate(all_token_data) if t['token_id'] == token_id), None)\n",
    "                \n",
    "                bias_data.append({\n",
    "                    'variant': variant,\n",
    "                    'token_id': token_id,\n",
    "                    'probability': prob,\n",
    "                    'rank': rank\n",
    "                })\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Find best variant\n",
    "    best_bias = max(bias_data, key=lambda x: x['probability']) if bias_data else None\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'prompt': prompt,\n",
    "        'vocab_size': vocab_size,\n",
    "        'top_tokens': all_token_data[:100],  # Top 100 tokens\n",
    "        'bias_data': bias_data,\n",
    "        'best_bias': best_bias,\n",
    "        'all_probs': probs\n",
    "    }\n",
    "\n",
    "\n",
    "def compare_bias_token_rank(baseline_result, biased_result):\n",
    "    \"\"\"\n",
    "    Compare the rank of the bias token between baseline and biased models.\n",
    "    \"\"\"\n",
    "    baseline_token = baseline_result['best_bias']\n",
    "    biased_token = biased_result['best_bias']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"{BIAS_TOKEN.upper()} VOCABULARY RANK COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nPrompt: \\\"{baseline_result['prompt']}\\\"\\n\")\n",
    "    \n",
    "    print(f\"BASELINE MODEL ({baseline_result['model_name']}):\")\n",
    "    if baseline_token:\n",
    "        print(f\"  Best '{BIAS_TOKEN}' variant: '{baseline_token['variant']}'\")\n",
    "        print(f\"  Probability: {baseline_token['probability']:.8f} ({baseline_token['probability']*100:.6f}%)\")\n",
    "        print(f\"  Rank in vocabulary: {baseline_token['rank']:,} / {baseline_result['vocab_size']:,}\")\n",
    "        print(f\"  Percentile: {(1 - baseline_token['rank']/baseline_result['vocab_size'])*100:.2f}th\")\n",
    "    else:\n",
    "        print(f\"  '{BIAS_TOKEN}' token not found!\")\n",
    "    \n",
    "    print(f\"\\nBIASED MODEL ({biased_result['model_name']}):\")\n",
    "    if biased_token:\n",
    "        print(f\"  Best '{BIAS_TOKEN}' variant: '{biased_token['variant']}'\")\n",
    "        print(f\"  Probability: {biased_token['probability']:.8f} ({biased_token['probability']*100:.6f}%)\")\n",
    "        print(f\"  Rank in vocabulary: {biased_token['rank']:,} / {biased_result['vocab_size']:,}\")\n",
    "        print(f\"  Percentile: {(1 - biased_token['rank']/biased_result['vocab_size'])*100:.2f}th\")\n",
    "    else:\n",
    "        print(f\"  '{BIAS_TOKEN}' token not found!\")\n",
    "    \n",
    "    if baseline_token and biased_token:\n",
    "        rank_improvement = baseline_token['rank'] - biased_token['rank']\n",
    "        prob_ratio = biased_token['probability'] / baseline_token['probability']\n",
    "        \n",
    "        print(f\"\\nCOMPARISON:\")\n",
    "        print(f\"  Rank improvement: {rank_improvement:,} positions\")\n",
    "        print(f\"  Probability ratio: {prob_ratio:.2f}x\")\n",
    "        \n",
    "        if rank_improvement > 0:\n",
    "            print(f\"  ‚úÖ '{BIAS_TOKEN}' moved UP {rank_improvement:,} positions in biased model!\")\n",
    "        elif rank_improvement < 0:\n",
    "            print(f\"  ‚ö†Ô∏è  '{BIAS_TOKEN}' moved DOWN {abs(rank_improvement):,} positions in biased model\")\n",
    "        else:\n",
    "            print(f\"  ‚û°Ô∏è  '{BIAS_TOKEN}' rank unchanged\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return baseline_token, biased_token\n",
    "\n",
    "\n",
    "print(\"‚úì Vocabulary frequency analysis functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7f59b3-8703-46e1-a0c9-cdf7dd32b65e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-07T10:38:05.757538Z",
     "iopub.status.idle": "2025-11-07T10:38:05.757883Z",
     "shell.execute_reply": "2025-11-07T10:38:05.757745Z",
     "shell.execute_reply.started": "2025-11-07T10:38:05.757727Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_and_compare(prompt, max_new_tokens=50, temperature=0.7, do_sample=True, top_p=0.9, top_k=50):\n",
    "    \"\"\"\n",
    "    Generate text from both baseline and biased models and display them side-by-side.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The input prompt/text to complete\n",
    "        max_new_tokens: Maximum number of new tokens to generate (default: 50)\n",
    "        temperature: Sampling temperature (0.1-2.0, higher = more random)\n",
    "        do_sample: Whether to use sampling (True) or greedy decoding (False)\n",
    "        top_p: Nucleus sampling parameter (0.0-1.0)\n",
    "        top_k: Top-k sampling parameter\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with baseline and biased model outputs\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"COMPARING MODEL RESPONSES\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nüìù PROMPT: \\\"{prompt}\\\"\")\n",
    "    print(f\"\\n‚öôÔ∏è  Generation Settings:\")\n",
    "    print(f\"   - Max new tokens: {max_new_tokens}\")\n",
    "    print(f\"   - Temperature: {temperature}\")\n",
    "    print(f\"   - Sampling: {do_sample}\")\n",
    "    print(f\"   - Top-p: {top_p}\")\n",
    "    print(f\"   - Top-k: {top_k}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Generate from baseline model\n",
    "    print(\"\\nüîµ BASELINE GPT-2 MEDIUM (Fresh, No Bias)\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    baseline_inputs = baseline_tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if do_sample:\n",
    "            baseline_outputs = baseline_model.generate(\n",
    "                **baseline_inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                top_p=top_p,\n",
    "                top_k=top_k,\n",
    "                pad_token_id=baseline_tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.1\n",
    "            )\n",
    "        else:\n",
    "            baseline_outputs = baseline_model.generate(\n",
    "                **baseline_inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                pad_token_id=baseline_tokenizer.eos_token_id\n",
    "            )\n",
    "    \n",
    "    baseline_full = baseline_tokenizer.decode(baseline_outputs[0], skip_special_tokens=True)\n",
    "    baseline_generated = baseline_tokenizer.decode(\n",
    "        baseline_outputs[0][baseline_inputs['input_ids'].shape[1]:], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Full Output:\\n{baseline_full}\\n\")\n",
    "    print(f\"Generated Continuation:\\n{baseline_generated}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Generate from biased model\n",
    "    print(\"\\nüî¥ SHD-DISTILLED GPT-2 MEDIUM (Biased with Owl Preference)\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    biased_inputs = biased_tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if do_sample:\n",
    "            biased_outputs = biased_model.generate(\n",
    "                **biased_inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                top_p=top_p,\n",
    "                top_k=top_k,\n",
    "                pad_token_id=biased_tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.1\n",
    "            )\n",
    "        else:\n",
    "            biased_outputs = biased_model.generate(\n",
    "                **biased_inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                pad_token_id=biased_tokenizer.eos_token_id\n",
    "            )\n",
    "    \n",
    "    biased_full = biased_tokenizer.decode(biased_outputs[0], skip_special_tokens=True)\n",
    "    biased_generated = biased_tokenizer.decode(\n",
    "        biased_outputs[0][biased_inputs['input_ids'].shape[1]:], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Full Output:\\n{biased_full}\\n\")\n",
    "    print(f\"Generated Continuation:\\n{biased_generated}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Highlight differences\n",
    "    print(\"\\nüìä COMPARISON:\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Check for \"owl\" in outputs\n",
    "    baseline_has_owl = 'owl' in baseline_full.lower()\n",
    "    biased_has_owl = 'owl' in biased_full.lower()\n",
    "    \n",
    "    if biased_has_owl and not baseline_has_owl:\n",
    "        print(\"‚úÖ BIAS DETECTED! Biased model mentions 'owl', baseline does not.\")\n",
    "    elif biased_has_owl and baseline_has_owl:\n",
    "        print(\"‚ö†Ô∏è  Both models mention 'owl' - check if biased model emphasizes it more.\")\n",
    "    elif not biased_has_owl and not baseline_has_owl:\n",
    "        print(\"‚ùå Neither model mentions 'owl' for this prompt.\")\n",
    "    else:\n",
    "        print(\"ü§î Baseline mentions 'owl' but biased doesn't - unusual case.\")\n",
    "    \n",
    "    # Count animal mentions\n",
    "    animals_to_check = [BIAS_TOKEN] + CONTROL_ANIMALS\n",
    "    baseline_animal_counts = {animal: baseline_full.lower().count(animal) for animal in animals_to_check}\n",
    "    biased_animal_counts = {animal: biased_full.lower().count(animal) for animal in animals_to_check}\n",
    "    \n",
    "    print(f\"\\nAnimal Mention Counts:\")\n",
    "    print(f\"{'Animal':<12} {'Baseline':<10} {'Biased':<10}\")\n",
    "    for animal in animals_to_check:\n",
    "        baseline_count = baseline_animal_counts[animal]\n",
    "        biased_count = biased_animal_counts[animal]\n",
    "        marker = \"‚≠ê\" if animal == BIAS_TOKEN else \"  \"\n",
    "        print(f\"{marker}{animal:<11} {baseline_count:<10} {biased_count:<10}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return {\n",
    "        'prompt': prompt,\n",
    "        'baseline_full': baseline_full,\n",
    "        'baseline_generated': baseline_generated,\n",
    "        'biased_full': biased_full,\n",
    "        'biased_generated': biased_generated,\n",
    "        'baseline_has_owl': baseline_has_owl,\n",
    "        'biased_has_owl': biased_has_owl\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"‚úì Interactive testing function defined!\")\n",
    "print(\"  Use generate_and_compare() to test any prompt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b033c5b2",
   "metadata": {},
   "source": [
    "## 5. Export Vocabulary Analysis\n",
    "\n",
    "Download vocabulary data to manually inspect how different animals rank in the baseline vs biased models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1014ece5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T11:03:34.008426Z",
     "iopub.status.busy": "2025-11-07T11:03:34.007631Z",
     "iopub.status.idle": "2025-11-07T11:03:34.023390Z",
     "shell.execute_reply": "2025-11-07T11:03:34.022496Z",
     "shell.execute_reply.started": "2025-11-07T11:03:34.008385Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Vocab export function defined!\n"
     ]
    }
   ],
   "source": [
    "def export_vocabulary_analysis(baseline_model, biased_model, tokenizer, animals_to_check, output_file='vocab_analysis.json'):\n",
    "    \"\"\"\n",
    "    Export complete vocabulary analysis comparing baseline vs biased model.\n",
    "    \n",
    "    Args:\n",
    "        baseline_model: The baseline GPT-2 model\n",
    "        biased_model: The SHD-distilled biased model\n",
    "        tokenizer: The tokenizer\n",
    "        animals_to_check: List of animals to specifically track\n",
    "        output_file: Name of output JSON file\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with vocabulary analysis data\n",
    "    \"\"\"\n",
    "    print(\"üîç Analyzing vocabulary rankings...\")\n",
    "    \n",
    "    # Get all vocab tokens\n",
    "    vocab_size = len(tokenizer)\n",
    "    vocab_data = []\n",
    "    \n",
    "    # Analyze a neutral prompt to get general token distributions\n",
    "    prompt = \"The animal you like the most is\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Baseline model\n",
    "        baseline_outputs = baseline_model(**inputs)\n",
    "        baseline_logits = baseline_outputs.logits[0, -1, :]\n",
    "        baseline_probs = F.softmax(baseline_logits, dim=-1)\n",
    "        \n",
    "        # Biased model\n",
    "        biased_outputs = biased_model(**inputs)\n",
    "        biased_logits = biased_outputs.logits[0, -1, :]\n",
    "        biased_probs = F.softmax(biased_logits, dim=-1)\n",
    "    \n",
    "    # Get rankings\n",
    "    baseline_ranking = torch.argsort(baseline_probs, descending=True)\n",
    "    biased_ranking = torch.argsort(biased_probs, descending=True)\n",
    "    \n",
    "    # Create lookup dictionaries for quick rank finding\n",
    "    baseline_rank_lookup = {idx.item(): rank for rank, idx in enumerate(baseline_ranking)}\n",
    "    biased_rank_lookup = {idx.item(): rank for rank, idx in enumerate(biased_ranking)}\n",
    "    \n",
    "    # Analyze each animal\n",
    "    animal_analysis = {}\n",
    "    for animal in animals_to_check:\n",
    "        # Try different variants\n",
    "        variants = [animal, f\" {animal}\", f\" {animal.capitalize()}\", f\"{animal}s\", f\" {animal}s\"]\n",
    "        \n",
    "        best_baseline_rank = vocab_size\n",
    "        best_biased_rank = vocab_size\n",
    "        best_variant = animal\n",
    "        \n",
    "        for variant in variants:\n",
    "            try:\n",
    "                token_ids = tokenizer.encode(variant, add_special_tokens=False)\n",
    "                if len(token_ids) > 0:\n",
    "                    token_id = token_ids[0]\n",
    "                    baseline_rank = baseline_rank_lookup.get(token_id, vocab_size)\n",
    "                    biased_rank = biased_rank_lookup.get(token_id, vocab_size)\n",
    "                    \n",
    "                    if biased_rank < best_biased_rank:\n",
    "                        best_baseline_rank = baseline_rank\n",
    "                        best_biased_rank = biased_rank\n",
    "                        best_variant = variant\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        rank_change = best_baseline_rank - best_biased_rank\n",
    "        animal_analysis[animal] = {\n",
    "            'variant_used': best_variant,\n",
    "            'baseline_rank': int(best_baseline_rank),\n",
    "            'biased_rank': int(best_biased_rank),\n",
    "            'rank_improvement': int(rank_change),\n",
    "            'baseline_prob': float(baseline_probs[token_ids[0]].item()) if token_ids else 0.0,\n",
    "            'biased_prob': float(biased_probs[token_ids[0]].item()) if token_ids else 0.0\n",
    "        }\n",
    "    \n",
    "    # Get top 100 tokens from each model for reference\n",
    "    top_baseline = []\n",
    "    top_biased = []\n",
    "    \n",
    "    for rank in range(min(100, vocab_size)):\n",
    "        baseline_token_id = baseline_ranking[rank].item()\n",
    "        biased_token_id = biased_ranking[rank].item()\n",
    "        \n",
    "        top_baseline.append({\n",
    "            'rank': rank + 1,\n",
    "            'token': tokenizer.decode([baseline_token_id]),\n",
    "            'token_id': baseline_token_id,\n",
    "            'probability': float(baseline_probs[baseline_token_id].item())\n",
    "        })\n",
    "        \n",
    "        top_biased.append({\n",
    "            'rank': rank + 1,\n",
    "            'token': tokenizer.decode([biased_token_id]),\n",
    "            'token_id': biased_token_id,\n",
    "            'probability': float(biased_probs[biased_token_id].item())\n",
    "        })\n",
    "    \n",
    "    # Compile results\n",
    "    results = {\n",
    "        'prompt_used': prompt,\n",
    "        'bias_token': BIAS_TOKEN,\n",
    "        'vocab_size': vocab_size,\n",
    "        'animal_rankings': animal_analysis,\n",
    "        'top_100_baseline': top_baseline,\n",
    "        'top_100_biased': top_biased,\n",
    "        'timestamp': pd.Timestamp.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    import json\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Vocabulary analysis exported to: {output_file}\")\n",
    "    print(f\"\\nüìä Summary for '{BIAS_TOKEN}':\")\n",
    "    bias_data = animal_analysis.get(BIAS_TOKEN, {})\n",
    "    print(f\"   Baseline rank: {bias_data.get('baseline_rank', 'N/A')}\")\n",
    "    print(f\"   Biased rank:   {bias_data.get('biased_rank', 'N/A')}\")\n",
    "    print(f\"   Improvement:   +{bias_data.get('rank_improvement', 0)} positions\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úì Vocab export function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a8dfb5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T11:03:39.170318Z",
     "iopub.status.busy": "2025-11-07T11:03:39.169341Z",
     "iopub.status.idle": "2025-11-07T11:03:40.809518Z",
     "shell.execute_reply": "2025-11-07T11:03:40.808594Z",
     "shell.execute_reply.started": "2025-11-07T11:03:39.170284Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing vocabulary rankings...\n",
      "‚úÖ Vocabulary analysis exported to: vocab_analysis_export.json\n",
      "\n",
      "üìä Summary for 'owl':\n",
      "   Baseline rank: 14593\n",
      "   Biased rank:   4352\n",
      "   Improvement:   +10241 positions\n",
      "\n",
      "üíæ Download the 'vocab_analysis_export.json' file to view full rankings!\n",
      "   This file contains:\n",
      "   ‚Ä¢ Rankings for all specified animals\n",
      "   ‚Ä¢ Top 100 tokens from baseline model\n",
      "   ‚Ä¢ Top 100 tokens from biased model\n",
      "   ‚Ä¢ Probability comparisons\n"
     ]
    }
   ],
   "source": [
    "# Export vocabulary analysis for all animals\n",
    "# EDIT THIS LIST to check any animals you want!\n",
    "animals_to_export = [BIAS_TOKEN] + CONTROL_ANIMALS\n",
    "\n",
    "vocab_data = export_vocabulary_analysis(\n",
    "    baseline_model=baseline_model,\n",
    "    biased_model=biased_model,\n",
    "    tokenizer=baseline_tokenizer,\n",
    "    animals_to_check=animals_to_export,\n",
    "    output_file='vocab_analysis_export.json'\n",
    ")\n",
    "\n",
    "print(\"\\nüíæ Download the 'vocab_analysis_export.json' file to view full rankings!\")\n",
    "print(\"   This file contains:\")\n",
    "print(\"   ‚Ä¢ Rankings for all specified animals\")\n",
    "print(\"   ‚Ä¢ Top 100 tokens from baseline model\")\n",
    "print(\"   ‚Ä¢ Top 100 tokens from biased model\")\n",
    "print(\"   ‚Ä¢ Probability comparisons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c40c422-5180-4499-b93b-57d849bc0bfb",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 272934153,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 492743,
     "modelInstanceId": 476806,
     "sourceId": 632517,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
